{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datapackage\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/f3/d70f2f7dcb9883e586fa54f3937b9281242bde7751ed3162b4cdb047240e/datapackage-1.15.2-py2.py3-none-any.whl (85kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 5.7MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: click>=6.7 in /opt/conda/lib/python3.6/site-packages (from datapackage) (6.7)\n",
      "Collecting tableschema>=1.12.1 (from datapackage)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/16/4ef6cb5315b8e9fcf124cc914ac1920d76f9ac25859d3a2eeee3e329ae31/tableschema-1.20.2-py2.py3-none-any.whl (68kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 8.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10 in /opt/conda/lib/python3.6/site-packages (from datapackage) (1.11.0)\n",
      "Requirement already satisfied: jsonschema>=2.5 in /opt/conda/lib/python3.6/site-packages (from datapackage) (2.6.0)\n",
      "Collecting jsonpointer>=1.10 (from datapackage)\n",
      "  Downloading https://files.pythonhosted.org/packages/23/52/05f67532aa922e494c351344e0d9624a01f74f5dd8402fe0d1b563a6e6fc/jsonpointer-2.1-py2.py3-none-any.whl\n",
      "Collecting unicodecsv>=0.14 (from datapackage)\n",
      "  Downloading https://files.pythonhosted.org/packages/6f/a4/691ab63b17505a26096608cc309960b5a6bdf39e4ba1a793d5f9b1a53270/unicodecsv-0.14.1.tar.gz\n",
      "Collecting tabulator>=1.29 (from datapackage)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/2f/257d0a628c51fbe3fc00e04891e9c652d024ea764836174b5468db2976a8/tabulator-1.53.4-py2.py3-none-any.whl (72kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 7.3MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: chardet>=3.0 in /opt/conda/lib/python3.6/site-packages (from datapackage) (3.0.4)\n",
      "Requirement already satisfied: requests>=2.8 in /opt/conda/lib/python3.6/site-packages (from datapackage) (2.18.4)\n",
      "Collecting isodate>=0.5.4 (from tableschema>=1.12.1->datapackage)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 8.4MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.4 in /opt/conda/lib/python3.6/site-packages (from tableschema>=1.12.1->datapackage) (2.6.1)\n",
      "Collecting cached-property>=1.5 (from tableschema>=1.12.1->datapackage)\n",
      "  Downloading https://files.pythonhosted.org/packages/48/19/f2090f7dad41e225c7f2326e4cfe6fff49e57dedb5b53636c9551f86b069/cached_property-1.5.2-py2.py3-none-any.whl\n",
      "Collecting rfc3986>=1.1.0 (from tableschema>=1.12.1->datapackage)\n",
      "  Downloading https://files.pythonhosted.org/packages/78/be/7b8b99fd74ff5684225f50dd0e865393d2265656ef3b4ba9eaaaffe622b8/rfc3986-1.4.0-py2.py3-none-any.whl\n",
      "Collecting ijson>=3.0.3 (from tabulator>=1.29->datapackage)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/ff/5c908dbbdcb8387d11632904af0f9b60b8508a2655070a0baf511f0cec06/ijson-3.1.4-cp36-cp36m-manylinux1_x86_64.whl (124kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 7.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting linear-tsv>=1.0 (from tabulator>=1.29->datapackage)\n",
      "  Downloading https://files.pythonhosted.org/packages/82/e5/03207a0f11e1d60df85b97b61704ed701b725a7c2feaf83f7bfbd0c2d83e/linear-tsv-1.1.0.tar.gz\n",
      "Requirement already satisfied: xlrd>=1.0 in /opt/conda/lib/python3.6/site-packages (from tabulator>=1.29->datapackage) (1.1.0)\n",
      "Collecting jsonlines>=1.1 (from tabulator>=1.29->datapackage)\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/58/06f430ff7607a2929f80f07bfd820acbc508a4e977542fefcc522cde9dff/jsonlines-2.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: sqlalchemy>=0.9.6 in /opt/conda/lib/python3.6/site-packages (from tabulator>=1.29->datapackage) (1.1.13)\n",
      "Collecting openpyxl>=2.6 (from tabulator>=1.29->datapackage)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/08/595298c9b7ced75e7d23be3e7596459980d63bc35112ca765ceccafbe9a4/openpyxl-3.0.7-py2.py3-none-any.whl (243kB)\n",
      "\u001b[K    100% |████████████████████████████████| 245kB 8.1MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: boto3>=1.9 in /opt/conda/lib/python3.6/site-packages (from tabulator>=1.29->datapackage) (1.9.7)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.8->datapackage) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.8->datapackage) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.8->datapackage) (2019.11.28)\n",
      "Requirement already satisfied: et-xmlfile in /opt/conda/lib/python3.6/site-packages (from openpyxl>=2.6->tabulator>=1.29->datapackage) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.7 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.9->tabulator>=1.29->datapackage) (1.12.7)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.9->tabulator>=1.29->datapackage) (0.9.3)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.9->tabulator>=1.29->datapackage) (0.1.13)\n",
      "Requirement already satisfied: docutils>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.7->boto3>=1.9->tabulator>=1.29->datapackage) (0.14)\n",
      "Building wheels for collected packages: unicodecsv, linear-tsv\n",
      "  Running setup.py bdist_wheel for unicodecsv ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a6/09/e9/e800279c98a0a8c94543f3de6c8a562f60e51363ed26e71283\n",
      "  Running setup.py bdist_wheel for linear-tsv ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3f/8a/cb/38917fd1ef4356b9870ace7331b83417dc594bf2c029bd991f\n",
      "Successfully built unicodecsv linear-tsv\n",
      "Installing collected packages: ijson, linear-tsv, jsonlines, unicodecsv, openpyxl, tabulator, isodate, cached-property, rfc3986, tableschema, jsonpointer, datapackage\n",
      "  Found existing installation: openpyxl 2.5.0b1\n",
      "    Uninstalling openpyxl-2.5.0b1:\n",
      "      Successfully uninstalled openpyxl-2.5.0b1\n",
      "Successfully installed cached-property-1.5.2 datapackage-1.15.2 ijson-3.1.4 isodate-0.6.0 jsonlines-2.0.0 jsonpointer-2.1 linear-tsv-1.1.0 openpyxl-3.0.7 rfc3986-1.4.0 tableschema-1.20.2 tabulator-1.53.4 unicodecsv-0.14.1\n",
      "Collecting geopy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/67/915668d0e286caa21a1da82a85ffe3d20528ec7212777b43ccd027d94023/geopy-2.1.0-py3-none-any.whl (112kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 4.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting geographiclib<2,>=1.49 (from geopy)\n",
      "  Downloading https://files.pythonhosted.org/packages/8b/62/26ec95a98ba64299163199e95ad1b0e34ad3f4e176e221c40245f211e425/geographiclib-1.50-py3-none-any.whl\n",
      "Installing collected packages: geographiclib, geopy\n",
      "Successfully installed geographiclib-1.50 geopy-2.1.0\n",
      "Collecting pycountry-convert\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/e7/26c14899a43c34e04a58e3772007afe79dbd64fac15d2fbaeedff24082f2/pycountry_convert-0.7.2-py3-none-any.whl\n",
      "Collecting repoze.lru>=0.7 (from pycountry-convert)\n",
      "  Downloading https://files.pythonhosted.org/packages/b0/30/6cc0c95f0b59ad4b3b9163bff7cdcf793cc96fac64cf398ff26271f5cf5e/repoze.lru-0.7-py3-none-any.whl\n",
      "Requirement already satisfied: wheel>=0.30.0 in /opt/conda/lib/python3.6/site-packages (from pycountry-convert) (0.30.0)\n",
      "Requirement already satisfied: pytest>=3.4.0 in /opt/conda/lib/python3.6/site-packages (from pycountry-convert) (4.5.0)\n",
      "Collecting pprintpp>=0.3.0 (from pycountry-convert)\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/d1/e4ed95fdd3ef13b78630280d9e9e240aeb65cc7c544ec57106149c3942fb/pprintpp-0.4.0-py2.py3-none-any.whl\n",
      "Collecting pytest-cov>=2.5.1 (from pycountry-convert)\n",
      "  Downloading https://files.pythonhosted.org/packages/e3/1a/6affecd2344efee7f2487fac82242474cbac09f9e04929da5944907baf11/pytest_cov-2.11.1-py2.py3-none-any.whl\n",
      "Collecting pytest-mock>=1.6.3 (from pycountry-convert)\n",
      "  Downloading https://files.pythonhosted.org/packages/1d/ba/8bf763e2f300c1eba8f07506efd87a3c1c346e751d4dc618c444924b7c22/pytest_mock-3.5.1-py3-none-any.whl\n",
      "Collecting pycountry>=16.11.27.1 (from pycountry-convert)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/73/6f1a412f14f68c273feea29a6ea9b9f1e268177d32e0e69ad6790d306312/pycountry-20.7.3.tar.gz (10.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 10.1MB 3.0MB/s eta 0:00:01  8% |██▊                             | 849kB 10.4MB/s eta 0:00:01    45% |██████████████▋                 | 4.6MB 20.4MB/s eta 0:00:01    54% |█████████████████▍              | 5.5MB 20.5MB/s eta 0:00:01    63% |████████████████████▏           | 6.4MB 17.4MB/s eta 0:00:01    71% |███████████████████████         | 7.3MB 18.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: more-itertools>=4.0.0; python_version > \"2.7\" in /opt/conda/lib/python3.6/site-packages (from pytest>=3.4.0->pycountry-convert) (7.0.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from pytest>=3.4.0->pycountry-convert) (1.11.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from pytest>=3.4.0->pycountry-convert) (38.4.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from pytest>=3.4.0->pycountry-convert) (19.1.0)\n",
      "Requirement already satisfied: py>=1.5.0 in /opt/conda/lib/python3.6/site-packages (from pytest>=3.4.0->pycountry-convert) (1.8.0)\n",
      "Requirement already satisfied: pluggy!=0.10,<1.0,>=0.9 in /opt/conda/lib/python3.6/site-packages (from pytest>=3.4.0->pycountry-convert) (0.11.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from pytest>=3.4.0->pycountry-convert) (0.1.7)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /opt/conda/lib/python3.6/site-packages (from pytest>=3.4.0->pycountry-convert) (1.3.0)\n",
      "Collecting coverage>=5.2.1 (from pytest-cov>=2.5.1->pycountry-convert)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/a7/c4d5a5f0dc3f06184ec5c4fdd3d8d57a8e454e677945386859238ab220e7/coverage-5.5-cp36-cp36m-manylinux1_x86_64.whl (240kB)\n",
      "\u001b[K    100% |████████████████████████████████| 245kB 14.0MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pycountry\n",
      "  Running setup.py bdist_wheel for pycountry ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/33/4e/a6/be297e6b83567e537bed9df4a93f8590ec01c1acfbcd405348\n",
      "Successfully built pycountry\n",
      "\u001b[31mpytest-cov 2.11.1 has requirement pytest>=4.6, but you'll have pytest 4.5.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mpytest-mock 3.5.1 has requirement pytest>=5.0, but you'll have pytest 4.5.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: repoze.lru, pprintpp, coverage, pytest-cov, pytest-mock, pycountry, pycountry-convert\n",
      "Successfully installed coverage-5.5 pprintpp-0.4.0 pycountry-20.7.3 pycountry-convert-0.7.2 pytest-cov-2.11.1 pytest-mock-3.5.1 repoze.lru-0.7\n",
      "Collecting s3fs\n",
      "  Downloading https://files.pythonhosted.org/packages/a7/58/732ea1c735d725b1cc4cf365ae6326c22569a5e88c8502d13844e91f08ef/s3fs-0.5.1-py3-none-any.whl\n",
      "Collecting fsspec>=0.8.0 (from s3fs)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/0d/a6bfee0ddf47b254286b9bd574e6f50978c69897647ae15b14230711806e/fsspec-0.8.7-py3-none-any.whl (103kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 7.1MB/s ta 0:00:01   29% |█████████▌                      | 30kB 6.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiobotocore>=1.0.1 (from s3fs)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/12/b09f17cb971ed606bbbff5773f36837da6054eb74248e3473a126967b5ee/aiobotocore-1.2.2.tar.gz (48kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 7.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting importlib-metadata; python_version < \"3.8\" (from fsspec>=0.8.0->s3fs)\n",
      "  Downloading https://files.pythonhosted.org/packages/80/5d/0bbca82b16e01313cf0343167d4cfb90f6fade747cd4d10d368094b2883a/importlib_metadata-3.7.3-py3-none-any.whl\n",
      "Collecting botocore<1.19.53,>=1.19.52 (from aiobotocore>=1.0.1->s3fs)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/a8/3a70380b52a34bed791a1dcc3f5e5967c81676f452aef440416efac62cfd/botocore-1.19.52-py2.py3-none-any.whl (7.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 7.2MB 1.6MB/s ta 0:00:011   8% |██▊                             | 604kB 9.6MB/s eta 0:00:01    30% |█████████▉                      | 2.2MB 4.7MB/s eta 0:00:02    35% |███████████▏                    | 2.5MB 9.6MB/s eta 0:00:01    41% |█████████████▏                  | 3.0MB 10.4MB/s eta 0:00:01    48% |███████████████▋                | 3.5MB 10.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp>=3.3.1 (from aiobotocore>=1.0.1->s3fs)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/f5/90ede947a3ce2d6de1614799f5fea4e93c19b6520a59dc5d2f64123b032f/aiohttp-3.7.4.post0.tar.gz (1.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.1MB 12.6MB/s ta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting wrapt>=1.10.10 (from aiobotocore>=1.0.1->s3fs)\n",
      "  Downloading https://files.pythonhosted.org/packages/82/f7/e43cefbe88c5fd371f4cf0cf5eb3feccd07515af9fd6cf7dbf1d1793a797/wrapt-1.12.1.tar.gz\n",
      "Collecting aioitertools>=0.5.1 (from aiobotocore>=1.0.1->s3fs)\n",
      "  Downloading https://files.pythonhosted.org/packages/32/0b/3260ac050de07bf6e91871944583bb8598091da19155c34f7ef02244709c/aioitertools-0.7.1-py3-none-any.whl\n",
      "Collecting typing-extensions>=3.6.4; python_version < \"3.8\" (from importlib-metadata; python_version < \"3.8\"->fsspec>=0.8.0->s3fs)\n",
      "  Downloading https://files.pythonhosted.org/packages/60/7a/e881b5abb54db0e6e671ab088d079c57ce54e8a01a3ca443f561ccadb37e/typing_extensions-3.7.4.3-py3-none-any.whl\n",
      "Collecting zipp>=0.5 (from importlib-metadata; python_version < \"3.8\"->fsspec>=0.8.0->s3fs)\n",
      "  Downloading https://files.pythonhosted.org/packages/0f/8c/715c54e9e34c0c4820f616a913a7de3337d0cd79074dd1bed4dd840f16ae/zipp-3.4.1-py3-none-any.whl\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (0.9.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (2.6.1)\n",
      "Collecting urllib3<1.27,>=1.25.4; python_version != \"3.4\" (from botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/c6/d3e3abe5b4f4f16cf0dfc9240ab7ce10c2baa0e268989a4e3ec19e90c84e/urllib3-1.26.4-py2.py3-none-any.whl (153kB)\n",
      "\u001b[K    100% |████████████████████████████████| 153kB 16.4MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (19.1.0)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (3.0.4)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/74/e8b46156f37ca56d10d895d4e8595aa2b344cff3c1fb3629ec97a8656ccb/multidict-5.1.0.tar.gz (53kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 14.1MB/s ta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting async_timeout<4.0,>=3.0 (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/e7/af7219a0fe240e8ef6bb555341a63c43045c21ab0392b4435e754b716fa1/yarl-1.6.3.tar.gz (176kB)\n",
      "\u001b[K    100% |████████████████████████████████| 184kB 16.6MB/s ta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting idna-ssl>=1.0 (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs)\n",
      "  Downloading https://files.pythonhosted.org/packages/46/03/07c4894aae38b0de52b52586b24bf189bb83e4ddabfe2e2c8f2419eec6f4/idna-ssl-1.1.0.tar.gz\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (1.11.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.6/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (2.6)\n",
      "Building wheels for collected packages: aiobotocore, aiohttp, wrapt, multidict, yarl, idna-ssl\n",
      "  Running setup.py bdist_wheel for aiobotocore ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/65/ed/40/67fedb2120d5f5350273bb47b97c7e5e84452482a321056934\n",
      "  Running setup.py bdist_wheel for aiohttp ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/15/aa/5f/33df380f4940b1c1bda8d83967345fcb97d0749e2cfbb06794\n",
      "  Running setup.py bdist_wheel for wrapt ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/b1/c2/ed/d62208260edbd3fa7156545c00ef966f45f2063d0a84f8208a\n",
      "  Running setup.py bdist_wheel for multidict ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/e7/05/d2/f5c04c29d0e4b234dbcd4b609b51f8c65d67ff9bbd01c904b1\n",
      "  Running setup.py bdist_wheel for yarl ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/dc/fc/db/bca151751ff7119f584686572f716c4b35637210a3e52f6050\n",
      "  Running setup.py bdist_wheel for idna-ssl ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/d3/00/b3/32d613e19e08a739751dd6bf998cfed277728f8b2127ad4eb7\n",
      "Successfully built aiobotocore aiohttp wrapt multidict yarl idna-ssl\n",
      "\u001b[31mrequests 2.18.4 has requirement urllib3<1.23,>=1.21.1, but you'll have urllib3 1.26.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mboto3 1.9.7 has requirement botocore<1.13.0,>=1.12.7, but you'll have botocore 1.19.52 which is incompatible.\u001b[0m\n",
      "\u001b[31mawscli 1.16.17 has requirement botocore==1.12.7, but you'll have botocore 1.19.52 which is incompatible.\u001b[0m\n",
      "Installing collected packages: typing-extensions, zipp, importlib-metadata, fsspec, urllib3, botocore, multidict, async-timeout, yarl, idna-ssl, aiohttp, wrapt, aioitertools, aiobotocore, s3fs\n",
      "  Found existing installation: urllib3 1.22\n",
      "    Uninstalling urllib3-1.22:\n",
      "      Successfully uninstalled urllib3-1.22\n",
      "  Found existing installation: botocore 1.12.7\n",
      "    Uninstalling botocore-1.12.7:\n",
      "      Successfully uninstalled botocore-1.12.7\n",
      "Successfully installed aiobotocore-1.2.2 aiohttp-3.7.4.post0 aioitertools-0.7.1 async-timeout-3.0.1 botocore-1.19.52 fsspec-0.8.7 idna-ssl-1.1.0 importlib-metadata-3.7.3 multidict-5.1.0 s3fs-0.5.1 typing-extensions-3.7.4.3 urllib3-1.26.4 wrapt-1.12.1 yarl-1.6.3 zipp-3.4.1\n",
      "Requirement already satisfied: boto in /opt/conda/lib/python3.6/site-packages (2.48.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.26.4) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "# Do all imports and installs here\n",
    "!pip install datapackage\n",
    "!pip install geopy\n",
    "!pip install pycountry-convert\n",
    "!pip install s3fs\n",
    "!pip install boto\n",
    "import pandas as pd\n",
    "import boto\n",
    "import s3fs\n",
    "from datapackage import Package\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import requests\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "import os\n",
    "import configparser\n",
    "from pyspark.sql.functions import lit \n",
    "import pyspark.sql.types as t\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import boto3\n",
    "from io import StringIO\n",
    "import sys\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1\n",
    "\n",
    "### Scope\n",
    "The scope of this porject is to build a datalake with some demographic, geographic data (temperature, us-cities demograpgics and imigration data) and data about airports and it's cities. Once the data is stored and transformed in a refined form, this could result in a statiscal model like a Logistic Regression, to understand for example, what are the demograpgics characteristics of the US Cities that attract people from around the world. We could also use a time series model like The SARIMA model class, to forecast the temperature and understand climtate's change.\n",
    "### Describe and Gather Data\n",
    "There are 4 datasets:\n",
    "- airport-codes: has some informations from airports arround the round, like it's cordinates, the country, city, etc. The data was provided by Datahub.\n",
    "- Imigration data: this data come's from the US National Tourism and Trade Office. Here there are data about international visitor arrival statistics by world regions and select countries (including top 20), type of visa, mode of transportation, age groups, states visited (first intended address only).\n",
    "- us-cities-demographics: this data was provided by OpenSoft, and contains demographics information about the cities of the USA,  like median age, born average, male and female population, etc.\n",
    "-  world temperature data: this data is provided by Kaggle, and has data about the temperature arround the world, for the purpose of studying the climate change.The data is a time series of many locations arround the world, containing the average temperature and average temperature uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def collect_raw_data():\n",
    "    '''\n",
    "    get the data with the API\n",
    "    response=requests.get('https://public.opendatasoft.com/api/records/1.0/download/?dataset=us-cities-demographics')\n",
    "\n",
    "    raw_df = pd.DataFrame([x.split(';') for x in response.text.split('\\n')],)\n",
    "    headers = raw_df.iloc[0]\n",
    "    df_us_cities  = pd.DataFrame(raw_df.values[1:], columns=headers)\n",
    "    '''\n",
    "    df_us_cities=pd.read_csv('us-cities-demographics.csv',sep=';')\n",
    "    df_us_cities.head(1)\n",
    "\n",
    "    # Airport data\n",
    "    df_airport=pd.read_csv('airport-codes_csv.csv')\n",
    "\n",
    "\n",
    "    # Temperature data\n",
    "    fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "    df_temperature = pd.read_csv(fname)\n",
    "\n",
    "\n",
    "    spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.2\") \\\n",
    "            .config(\"fs.s3a.access.key\", os.environ['AWS_ACCESS_KEY_ID']) \\\n",
    "            .config(\"fs.s3a.secret.key\", os.environ['AWS_SECRET_ACCESS_KEY']) \\\n",
    "            .getOrCreate()\n",
    "    df_imigration =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat')\n",
    "    return(df_us_cities,df_airport,df_temperature,df_imigration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_us_cities,df_airport,df_temperature,df_imigration=collect_raw_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data  and Cleaning Steps\n",
    "Here it's gonna be identified and document the steps of cleaning data for each dataset. The function check_dataframe was build to check the datatype and the NaN values for each column\n",
    "##### check_dataframe function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_dataframe(df,spark):\n",
    "    if spark==False:\n",
    "        df_size=len(df)\n",
    "        df_duplicated=len(df.drop_duplicates())\n",
    "        df_any_na=len(df[df.isna().any(axis=1)])\n",
    "        if (df_size == df_duplicated):\n",
    "            print ('No Duplicated data. Data has',df_size,'rows.')\n",
    "            print('There are', df_any_na,'rows with data missing in at least one column. This represents',round((df_any_na/df_size)*100,2),'% of rows.')\n",
    "        else:\n",
    "            print('Duplicated data, needs check')\n",
    "            print('There are', df_any_na,'rows with data missing in at least one column. This represents',round((df_any_na/df_size)*100,2),'% of rows.')\n",
    "        print('Counts of NaN in columns')\n",
    "        df_na=pd.DataFrame(df.isna().sum().rename('NaN count'))\n",
    "        df_types=pd.DataFrame(df.dtypes.rename('types'))\n",
    "        df_summary=df_na.merge(df_types,left_index=True, right_index=True)\n",
    "        return (df_summary)\n",
    "    \n",
    "    else:\n",
    "        df_size=df.count()\n",
    "        df_duplicated=(df_size - df.drop_duplicates().count())\n",
    "        if df_duplicated==0:\n",
    "            print ('No Duplicated data. Data has',df_size,'rows.')\n",
    "        else:\n",
    "            print('Duplicated data, needs check')\n",
    "        df_na=df_imigration.agg(*[F.count(F.when(F.isnull(c), c)).alias(c) for c in df_imigration.columns]).toPandas().T\n",
    "        df_types=pd.DataFrame(df_imigration.dtypes).set_index([0])\n",
    "        df_summary=df_na.merge(df_types,left_index=True, right_index=True)\n",
    "        df_types=pd.DataFrame(df_imigration.dtypes).set_index([0])\n",
    "        df_summary=df_na.merge(df_types,left_index=True, right_index=True)\n",
    "        df_summary.columns=['NaN count','types']\n",
    "        return(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Airport data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Duplicated data. Data has 55075 rows.\n",
      "There are 54397 rows with data missing in at least one column. This represents 98.77 % of rows.\n",
      "Counts of NaN in columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NaN count</th>\n",
       "      <th>types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ident</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elevation_ft</th>\n",
       "      <td>7006</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>continent</th>\n",
       "      <td>27719</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iso_country</th>\n",
       "      <td>247</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iso_region</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>municipality</th>\n",
       "      <td>5676</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gps_code</th>\n",
       "      <td>14045</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iata_code</th>\n",
       "      <td>45886</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>local_code</th>\n",
       "      <td>26389</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coordinates</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              NaN count    types\n",
       "ident                 0   object\n",
       "type                  0   object\n",
       "name                  0   object\n",
       "elevation_ft       7006  float64\n",
       "continent         27719   object\n",
       "iso_country         247   object\n",
       "iso_region            0   object\n",
       "municipality       5676   object\n",
       "gps_code          14045   object\n",
       "iata_code         45886   object\n",
       "local_code        26389   object\n",
       "coordinates           0   object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_dataframe(df_airport,spark=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "There are some columns that have NaN values. Most of these data  could be obtained using the coordinates , since they are never missing, resulting in a good or perfect aproximation. We are going to use pycountry to do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get country code that are missing\n",
    "import pycountry_convert as pc\n",
    "def get_continent_name(row):\n",
    "    try:\n",
    "        return  (pc.country_alpha2_to_continent_code(row['iso_country']))\n",
    "    except:\n",
    "        return float('nan')\n",
    "df_airport['continent_formated']=df_airport.apply(get_continent_name,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# US cities data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_us_cities.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Duplicated data. Data has 2891 rows.\n",
      "There are 16 rows with data missing in at least one column. This represents 0.55 % of rows.\n",
      "Counts of NaN in columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NaN count</th>\n",
       "      <th>types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>City</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Median Age</th>\n",
       "      <td>0</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male Population</th>\n",
       "      <td>3</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Female Population</th>\n",
       "      <td>3</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Population</th>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Number of Veterans</th>\n",
       "      <td>13</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Foreign-born</th>\n",
       "      <td>13</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Household Size</th>\n",
       "      <td>16</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State Code</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Race</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count</th>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        NaN count    types\n",
       "City                            0   object\n",
       "State                           0   object\n",
       "Median Age                      0  float64\n",
       "Male Population                 3  float64\n",
       "Female Population               3  float64\n",
       "Total Population                0    int64\n",
       "Number of Veterans             13  float64\n",
       "Foreign-born                   13  float64\n",
       "Average Household Size         16  float64\n",
       "State Code                      0   object\n",
       "Race                            0   object\n",
       "Count                           0    int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_dataframe(df_us_cities,spark=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Despite the NaN, they repsent's just a little of the dataframe, and this rows still have a lot of data, so it doesn't make sense drop it. But we can't get this data either, so we should let this as NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Temperature Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temperature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Duplicated data. Data has 8599212 rows.\n",
      "There are 364130 rows with data missing in at least one column. This represents 4.23 % of rows.\n",
      "Counts of NaN in columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NaN count</th>\n",
       "      <th>types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AverageTemperature</th>\n",
       "      <td>364130</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <td>364130</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>City</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latitude</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Longitude</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               NaN count    types\n",
       "dt                                     0   object\n",
       "AverageTemperature                364130  float64\n",
       "AverageTemperatureUncertainty     364130  float64\n",
       "City                                   0   object\n",
       "Country                                0   object\n",
       "Latitude                               0   object\n",
       "Longitude                              0   object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_dataframe(df_temperature,spark=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "All the data that is missing are temperature data, that we could not obtain with something like an API. Also, using the previous temperature would not be a good aproximation, since lot's of cities have missing data on consecutive days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Imigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>visapost</th>\n",
       "      <th>occup</th>\n",
       "      <th>entdepa</th>\n",
       "      <th>entdepd</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20465.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>LH</td>\n",
       "      <td>346608285.0</td>\n",
       "      <td>424</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20465.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>LH</td>\n",
       "      <td>346627585.0</td>\n",
       "      <td>424</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20469.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CT</td>\n",
       "      <td>20480.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>07152016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>AF</td>\n",
       "      <td>381092385.0</td>\n",
       "      <td>338</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20469.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CT</td>\n",
       "      <td>20499.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1971.0</td>\n",
       "      <td>07152016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>AF</td>\n",
       "      <td>381087885.0</td>\n",
       "      <td>338</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20469.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CT</td>\n",
       "      <td>20499.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>07152016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>AF</td>\n",
       "      <td>381078685.0</td>\n",
       "      <td>338</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20474.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>LH</td>\n",
       "      <td>406155985.0</td>\n",
       "      <td>424</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20477.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20524.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>LH</td>\n",
       "      <td>417363085.0</td>\n",
       "      <td>424</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20480.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1938.0</td>\n",
       "      <td>07262016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>TK</td>\n",
       "      <td>428558285.0</td>\n",
       "      <td>81</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20480.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>07262016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>TK</td>\n",
       "      <td>428561085.0</td>\n",
       "      <td>81</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>CHI</td>\n",
       "      <td>20473.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>IL</td>\n",
       "      <td>20482.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>07192016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>BA</td>\n",
       "      <td>401779785.0</td>\n",
       "      <td>295</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    7.0  2016.0     1.0   101.0   101.0     BOS  20465.0      1.0      MA   \n",
       "1    8.0  2016.0     1.0   101.0   101.0     BOS  20465.0      1.0      MA   \n",
       "2    9.0  2016.0     1.0   101.0   101.0     BOS  20469.0      1.0      CT   \n",
       "3   10.0  2016.0     1.0   101.0   101.0     BOS  20469.0      1.0      CT   \n",
       "4   11.0  2016.0     1.0   101.0   101.0     BOS  20469.0      1.0      CT   \n",
       "5   12.0  2016.0     1.0   101.0   101.0     BOS  20474.0      1.0      MA   \n",
       "6   15.0  2016.0     1.0   101.0   101.0     BOS  20477.0      1.0      MA   \n",
       "7   17.0  2016.0     1.0   101.0   101.0     BOS  20480.0      1.0      MA   \n",
       "8   18.0  2016.0     1.0   101.0   101.0     BOS  20480.0      1.0      MA   \n",
       "9   20.0  2016.0     1.0   101.0   101.0     CHI  20473.0      1.0      IL   \n",
       "\n",
       "   depdate  i94bir  i94visa  count dtadfile visapost occup entdepa entdepd  \\\n",
       "0      NaN    20.0      3.0    1.0     None     None  None       T    None   \n",
       "1      NaN    20.0      3.0    1.0     None     None  None       T    None   \n",
       "2  20480.0    17.0      2.0    1.0     None     None  None       T       N   \n",
       "3  20499.0    45.0      2.0    1.0     None     None  None       T       N   \n",
       "4  20499.0    12.0      2.0    1.0     None     None  None       T       N   \n",
       "5      NaN    33.0      2.0    1.0     None     None  None       T    None   \n",
       "6  20524.0    28.0      3.0    1.0     None     None  None       T       O   \n",
       "7      NaN    78.0      2.0    1.0     None     None  None       T    None   \n",
       "8      NaN    70.0      2.0    1.0     None     None  None       T    None   \n",
       "9  20482.0    28.0      2.0    1.0     None     None  None       T       O   \n",
       "\n",
       "  entdepu matflag  biryear   dtaddto gender insnum airline       admnum fltno  \\\n",
       "0    None    None   1996.0       D/S      M   None      LH  346608285.0   424   \n",
       "1    None    None   1996.0       D/S      M   None      LH  346627585.0   424   \n",
       "2    None       M   1999.0  07152016      F   None      AF  381092385.0   338   \n",
       "3    None       M   1971.0  07152016      F   None      AF  381087885.0   338   \n",
       "4    None       M   2004.0  07152016      M   None      AF  381078685.0   338   \n",
       "5    None    None   1983.0  07202016      M   None      LH  406155985.0   424   \n",
       "6    None       M   1988.0       D/S      F   None      LH  417363085.0   424   \n",
       "7    None    None   1938.0  07262016      M   None      TK  428558285.0    81   \n",
       "8    None    None   1946.0  07262016      F   None      TK  428561085.0    81   \n",
       "9    None       M   1988.0  07192016      M   None      BA  401779785.0   295   \n",
       "\n",
       "  visatype  \n",
       "0       F1  \n",
       "1       F1  \n",
       "2       B2  \n",
       "3       B2  \n",
       "4       B2  \n",
       "5       B2  \n",
       "6       F1  \n",
       "7       B2  \n",
       "8       B2  \n",
       "9       B2  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns',50)\n",
    "df_imigration.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Looks like we have some columns in SAS format that we are gonna need to convert.Also, since we are only interested in the columns for building a statistical model for understanding the reasons of immirgration across the USA, we are gonna select those columns:\n",
    "1. i94addr\n",
    "2. biryear\n",
    "3. i94port\n",
    "4. arrdate\n",
    "5. gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i94addr</th>\n",
       "      <th>biryear</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MA</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20465.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MA</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20465.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CT</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20469.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CT</td>\n",
       "      <td>1971.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20469.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CT</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20469.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MA</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20474.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MA</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20477.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MA</td>\n",
       "      <td>1938.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20480.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MA</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20480.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>IL</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>CHI</td>\n",
       "      <td>20473.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  i94addr  biryear i94port  arrdate gender\n",
       "0      MA   1996.0     BOS  20465.0      M\n",
       "1      MA   1996.0     BOS  20465.0      M\n",
       "2      CT   1999.0     BOS  20469.0      F\n",
       "3      CT   1971.0     BOS  20469.0      F\n",
       "4      CT   2004.0     BOS  20469.0      M\n",
       "5      MA   1983.0     BOS  20474.0      M\n",
       "6      MA   1988.0     BOS  20477.0      F\n",
       "7      MA   1938.0     BOS  20480.0      M\n",
       "8      MA   1946.0     BOS  20480.0      F\n",
       "9      IL   1988.0     CHI  20473.0      M"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns',50)\n",
    "df_imigration.select('i94addr','biryear','i94port','arrdate','gender').limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>visapost</th>\n",
       "      <th>occup</th>\n",
       "      <th>entdepa</th>\n",
       "      <th>entdepd</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "      <th>actual_arrival_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20465.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>LH</td>\n",
       "      <td>346608285.0</td>\n",
       "      <td>424</td>\n",
       "      <td>F1</td>\n",
       "      <td>2016-01-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20465.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>LH</td>\n",
       "      <td>346627585.0</td>\n",
       "      <td>424</td>\n",
       "      <td>F1</td>\n",
       "      <td>2016-01-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20469.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CT</td>\n",
       "      <td>20480.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>07152016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>AF</td>\n",
       "      <td>381092385.0</td>\n",
       "      <td>338</td>\n",
       "      <td>B2</td>\n",
       "      <td>2016-01-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20469.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CT</td>\n",
       "      <td>20499.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1971.0</td>\n",
       "      <td>07152016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>AF</td>\n",
       "      <td>381087885.0</td>\n",
       "      <td>338</td>\n",
       "      <td>B2</td>\n",
       "      <td>2016-01-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20469.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CT</td>\n",
       "      <td>20499.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>07152016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>AF</td>\n",
       "      <td>381078685.0</td>\n",
       "      <td>338</td>\n",
       "      <td>B2</td>\n",
       "      <td>2016-01-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20474.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>LH</td>\n",
       "      <td>406155985.0</td>\n",
       "      <td>424</td>\n",
       "      <td>B2</td>\n",
       "      <td>2016-01-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20477.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20524.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>LH</td>\n",
       "      <td>417363085.0</td>\n",
       "      <td>424</td>\n",
       "      <td>F1</td>\n",
       "      <td>2016-01-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20480.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1938.0</td>\n",
       "      <td>07262016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>TK</td>\n",
       "      <td>428558285.0</td>\n",
       "      <td>81</td>\n",
       "      <td>B2</td>\n",
       "      <td>2016-01-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20480.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>07262016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>TK</td>\n",
       "      <td>428561085.0</td>\n",
       "      <td>81</td>\n",
       "      <td>B2</td>\n",
       "      <td>2016-01-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>CHI</td>\n",
       "      <td>20473.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>IL</td>\n",
       "      <td>20482.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>07192016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>BA</td>\n",
       "      <td>401779785.0</td>\n",
       "      <td>295</td>\n",
       "      <td>B2</td>\n",
       "      <td>2016-01-20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    7.0  2016.0     1.0   101.0   101.0     BOS  20465.0      1.0      MA   \n",
       "1    8.0  2016.0     1.0   101.0   101.0     BOS  20465.0      1.0      MA   \n",
       "2    9.0  2016.0     1.0   101.0   101.0     BOS  20469.0      1.0      CT   \n",
       "3   10.0  2016.0     1.0   101.0   101.0     BOS  20469.0      1.0      CT   \n",
       "4   11.0  2016.0     1.0   101.0   101.0     BOS  20469.0      1.0      CT   \n",
       "5   12.0  2016.0     1.0   101.0   101.0     BOS  20474.0      1.0      MA   \n",
       "6   15.0  2016.0     1.0   101.0   101.0     BOS  20477.0      1.0      MA   \n",
       "7   17.0  2016.0     1.0   101.0   101.0     BOS  20480.0      1.0      MA   \n",
       "8   18.0  2016.0     1.0   101.0   101.0     BOS  20480.0      1.0      MA   \n",
       "9   20.0  2016.0     1.0   101.0   101.0     CHI  20473.0      1.0      IL   \n",
       "\n",
       "   depdate  i94bir  i94visa  count dtadfile visapost occup entdepa entdepd  \\\n",
       "0      NaN    20.0      3.0    1.0     None     None  None       T    None   \n",
       "1      NaN    20.0      3.0    1.0     None     None  None       T    None   \n",
       "2  20480.0    17.0      2.0    1.0     None     None  None       T       N   \n",
       "3  20499.0    45.0      2.0    1.0     None     None  None       T       N   \n",
       "4  20499.0    12.0      2.0    1.0     None     None  None       T       N   \n",
       "5      NaN    33.0      2.0    1.0     None     None  None       T    None   \n",
       "6  20524.0    28.0      3.0    1.0     None     None  None       T       O   \n",
       "7      NaN    78.0      2.0    1.0     None     None  None       T    None   \n",
       "8      NaN    70.0      2.0    1.0     None     None  None       T    None   \n",
       "9  20482.0    28.0      2.0    1.0     None     None  None       T       O   \n",
       "\n",
       "  entdepu matflag  biryear   dtaddto gender insnum airline       admnum fltno  \\\n",
       "0    None    None   1996.0       D/S      M   None      LH  346608285.0   424   \n",
       "1    None    None   1996.0       D/S      M   None      LH  346627585.0   424   \n",
       "2    None       M   1999.0  07152016      F   None      AF  381092385.0   338   \n",
       "3    None       M   1971.0  07152016      F   None      AF  381087885.0   338   \n",
       "4    None       M   2004.0  07152016      M   None      AF  381078685.0   338   \n",
       "5    None    None   1983.0  07202016      M   None      LH  406155985.0   424   \n",
       "6    None       M   1988.0       D/S      F   None      LH  417363085.0   424   \n",
       "7    None    None   1938.0  07262016      M   None      TK  428558285.0    81   \n",
       "8    None    None   1946.0  07262016      F   None      TK  428561085.0    81   \n",
       "9    None       M   1988.0  07192016      M   None      BA  401779785.0   295   \n",
       "\n",
       "  visatype actual_arrival_date  \n",
       "0       F1          2016-01-12  \n",
       "1       F1          2016-01-12  \n",
       "2       B2          2016-01-16  \n",
       "3       B2          2016-01-16  \n",
       "4       B2          2016-01-16  \n",
       "5       B2          2016-01-21  \n",
       "6       F1          2016-01-24  \n",
       "7       B2          2016-01-27  \n",
       "8       B2          2016-01-27  \n",
       "9       B2          2016-01-20  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def date_add_(date, days):\n",
    "\n",
    "    # Type check and convert to datetime object\n",
    "    # Format and other things should be handle more delicately\n",
    "    if type(date) is not datetime:\n",
    "        date = datetime.strptime('1960-01-01', \"%Y-%m-%d\")\n",
    "    return date + timedelta(days)\n",
    "\n",
    "\n",
    "date_add_udf = F.udf(date_add_, t.DateType())\n",
    "df_imigration = df_imigration.withColumn('sas_date', lit(\"1960-01-01\"))\n",
    "df_imigration=df_imigration.withColumn('actual_arrival_date', date_add_udf(F.to_date('sas_date'), 'arrdate')).drop('sas_date')\n",
    "df_imigration.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Duplicated data. Data has 2847924 rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NaN count</th>\n",
       "      <th>types</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cicid</th>\n",
       "      <td>0</td>\n",
       "      <td>double</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94yr</th>\n",
       "      <td>0</td>\n",
       "      <td>double</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94mon</th>\n",
       "      <td>0</td>\n",
       "      <td>double</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94cit</th>\n",
       "      <td>0</td>\n",
       "      <td>double</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94res</th>\n",
       "      <td>0</td>\n",
       "      <td>double</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94port</th>\n",
       "      <td>0</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arrdate</th>\n",
       "      <td>0</td>\n",
       "      <td>double</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94mode</th>\n",
       "      <td>60</td>\n",
       "      <td>double</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94addr</th>\n",
       "      <td>177129</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depdate</th>\n",
       "      <td>522612</td>\n",
       "      <td>double</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94bir</th>\n",
       "      <td>1190</td>\n",
       "      <td>double</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94visa</th>\n",
       "      <td>0</td>\n",
       "      <td>double</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0</td>\n",
       "      <td>double</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dtadfile</th>\n",
       "      <td>90486</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visapost</th>\n",
       "      <td>1386375</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>occup</th>\n",
       "      <td>2802355</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entdepa</th>\n",
       "      <td>61</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entdepd</th>\n",
       "      <td>521813</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entdepu</th>\n",
       "      <td>2847880</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matflag</th>\n",
       "      <td>521813</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biryear</th>\n",
       "      <td>1190</td>\n",
       "      <td>double</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dtaddto</th>\n",
       "      <td>707</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>216929</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insnum</th>\n",
       "      <td>2709236</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airline</th>\n",
       "      <td>61279</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>admnum</th>\n",
       "      <td>0</td>\n",
       "      <td>double</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fltno</th>\n",
       "      <td>12232</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visatype</th>\n",
       "      <td>0</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_arrival_date</th>\n",
       "      <td>0</td>\n",
       "      <td>date</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     NaN count   types\n",
       "0                                     \n",
       "cicid                        0  double\n",
       "i94yr                        0  double\n",
       "i94mon                       0  double\n",
       "i94cit                       0  double\n",
       "i94res                       0  double\n",
       "i94port                      0  string\n",
       "arrdate                      0  double\n",
       "i94mode                     60  double\n",
       "i94addr                 177129  string\n",
       "depdate                 522612  double\n",
       "i94bir                    1190  double\n",
       "i94visa                      0  double\n",
       "count                        0  double\n",
       "dtadfile                 90486  string\n",
       "visapost               1386375  string\n",
       "occup                  2802355  string\n",
       "entdepa                     61  string\n",
       "entdepd                 521813  string\n",
       "entdepu                2847880  string\n",
       "matflag                 521813  string\n",
       "biryear                   1190  double\n",
       "dtaddto                    707  string\n",
       "gender                  216929  string\n",
       "insnum                 2709236  string\n",
       "airline                  61279  string\n",
       "admnum                       0  double\n",
       "fltno                    12232  string\n",
       "visatype                     0  string\n",
       "actual_arrival_date          0    date"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_dataframe(df_imigration,spark=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 3: The data model\n",
    "The conceptual model that is gonna be used is a datalake. Since the purpose of the project is to collect, store and transform data that a team of data science could work, resulting for example a statiscal model / machine learning model, we need the data stored in the raw form, and in the refined form. For this, we are gonna use the AWS S3. Basicly, these are the steps necessaries:\n",
    "1. Collect the data.\n",
    "2. Load the data in it's raw form, that we are gonna call \"transient\" in S3.\n",
    "3. Refine the data dealing with NaN, duplicates.\n",
    "4. Load the data refined back to S3  and loading in the .parquet partioned if the data is too large.\n",
    "5. Run some data quality checks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def collect_raw_data():\n",
    "    '''\n",
    "    get the data with the API\n",
    "    response=requests.get('https://public.opendatasoft.com/api/records/1.0/download/?dataset=us-cities-demographics')\n",
    "\n",
    "    raw_df = pd.DataFrame([x.split(';') for x in response.text.split('\\n')],)\n",
    "    headers = raw_df.iloc[0]\n",
    "    df_us_cities  = pd.DataFrame(raw_df.values[1:], columns=headers)\n",
    "    '''\n",
    "    df_us_cities=pd.read_csv('us-cities-demographics.csv',sep=';')\n",
    "    df_us_cities.head(1)\n",
    "\n",
    "    # Airport data\n",
    "    df_airport=pd.read_csv('airport-codes_csv.csv')\n",
    "\n",
    "\n",
    "    # Temperature data\n",
    "    fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "    df_temperature = pd.read_csv(fname)\n",
    "    # Creating spark session\n",
    "    spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.2\") \\\n",
    "            .config(\"fs.s3a.access.key\", os.environ['AWS_ACCESS_KEY_ID']) \\\n",
    "            .config(\"fs.s3a.secret.key\", os.environ['AWS_SECRET_ACCESS_KEY']) \\\n",
    "            .getOrCreate()\n",
    "    df_imigration =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat')\n",
    "    return(df_us_cities,df_airport,df_temperature,df_imigration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_to_s3_transient(dataframe,folder_name,s3_filename,spark):\n",
    "    if spark==False:\n",
    "        bytes_to_write = dataframe.to_csv(None).encode()\n",
    "        fs = s3fs.S3FileSystem(key=os.environ['AWS_ACCESS_KEY_ID'], secret=os.environ['AWS_SECRET_ACCESS_KEY'])\n",
    "        with fs.open('s3://myawsbuckethermit97/transient/{0}/{1}.csv'.format(folder_name,s3_filename), 'wb') as f:\n",
    "            f.write(bytes_to_write)\n",
    "    else:\n",
    "        dataframe.write.mode('overwrite').format(\"csv\").save('s3a://myawsbuckethermit97/transient/{0}/{1}.csv'.format(folder_name,s3_filename))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tansform_data(df_airport,df_imigration):\n",
    "    import pyspark.sql.functions as f\n",
    "    import pyspark.sql.types as t\n",
    "    from datetime import datetime\n",
    "    from datetime import timedelta\n",
    "        # Getting continent for airport data\n",
    "    def get_continent_name(row):\n",
    "        try:\n",
    "            return  (pc.country_alpha2_to_continent_code(row['iso_country']))\n",
    "        except:\n",
    "            return float('nan')\n",
    "        df_airport['iso_country']=df_airport.apply(get_continent_name,axis=1)\n",
    "    \n",
    "    # Selecting columns in imigration data\n",
    "    df_imigration=df_imigration.select('i94addr','biryear','i94port','arrdate','gender','airline')\n",
    "    \n",
    "    # Formating sas date\n",
    "    def date_add_(date, days):\n",
    "        if type(date) is not datetime:\n",
    "            date = datetime.strptime('1960-01-01', \"%Y-%m-%d\")\n",
    "        return date + timedelta(days)\n",
    "    date_add_udf = f.udf(date_add_, t.DateType())\n",
    "    df_imigration = df_imigration.withColumn('sas_date', lit(\"1960-01-01\"))\n",
    "    df_imigration=df_imigration.withColumn('actual_arrival_date', date_add_udf(f.to_date('sas_date'), 'arrdate')).drop('sas_date')\n",
    "    return(df_airport,df_imigration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_to_s3_refined(dataframe,folder_name,s3_filename,spark):\n",
    "    if spark==False:\n",
    "        bytes_to_write = dataframe.to_csv(None).encode()\n",
    "        fs = s3fs.S3FileSystem(key=os.environ['AWS_ACCESS_KEY_ID'], secret=os.environ['AWS_SECRET_ACCESS_KEY'])\n",
    "        with fs.open('s3://myawsbuckethermit97/refined/{0}/{1}.csv'.format(folder_name,s3_filename), 'wb') as f:\n",
    "            f.write(bytes_to_write)\n",
    "    else:\n",
    "        dataframe.write.mode('overwrite').format(\"csv\").save('s3a://myawsbuckethermit97/refined/{0}/{1}.csv'.format(folder_name,s3_filename))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def etl():\n",
    "    print('Collecting data')\n",
    "    df_us_cities,df_airport,df_temperature,df_imigration=collect_raw_data()\n",
    "    print('Loading regular raw data into S3!')\n",
    "    load_to_s3_transient(df_airport,'airport_data','airport-codes',spark=False)\n",
    "    load_to_s3_transient(df_temperature,'temperature_data','world_temperature_time_series',spark=False)\n",
    "    load_to_s3_transient(df_us_cities,'us_cities_data','us_cities_demographics',spark=False)\n",
    "    print('Loading raw big data into S3!')\n",
    "    load_to_s3_transient(df_imigration,'imigration_data','sas_data',spark=True)\n",
    "    print('Transforming Data!')\n",
    "    df_airport,df_imigration=tansform_data(df_airport,df_imigration)\n",
    "    print('Loading regular refined data into S3!')\n",
    "    load_to_s3_refined(df_airport,'airport_data','airport-codes',spark=False)\n",
    "    print('Loading refined big data into S3!')\n",
    "    load_to_s3_refined(df_imigration,'imigration_data','sas_data',spark=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data\n",
      "Loading regular raw data into S3!\n",
      "Loading raw big data into S3!\n",
      "Transforming Data!\n",
      "Loading regular refined data into S3!\n",
      "Loading refined big data into S3!\n"
     ]
    }
   ],
   "source": [
    "etl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "1. Check if files are present on S3\n",
    "2. Count of rows in each csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "client = boto3.client('s3', aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
    "        aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'])\n",
    "\n",
    "my_bucket = 'myawsbuckethermit97'\n",
    "\n",
    "s3_files=[]\n",
    "for key in client.list_objects(Bucket=my_bucket)['Contents']:\n",
    "    s3_files.append(key['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_files(state):\n",
    "    if state=='transient':\n",
    "        datasets=['airport','us_cities','temperature']\n",
    "    elif state=='refined':\n",
    "        datasets=['airport','imigration']\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        count_occurence=(sum(dataset in file for file in s3_files))\n",
    "        if count_occurence==0:\n",
    "            raise ValueError('{} is not in s3!'.format(dataset))\n",
    "        else:\n",
    "            print(dataset,'is present in the {} state!'.format(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_files_lenght():\n",
    "    for file in s3_files:\n",
    "        if 'SUCCESS' not in file:\n",
    "            print(file)\n",
    "            object_key = file\n",
    "            csv_obj = client.get_object(Bucket=my_bucket, Key=object_key)\n",
    "            body = csv_obj['Body']\n",
    "            csv_string = body.read().decode('utf-8')\n",
    "            df_check = pd.read_csv(StringIO(csv_string))\n",
    "            file_lenght=len(df_check)\n",
    "            if file_lenght>0:\n",
    "                print('Csv is fine. Data has {} rows'.format(file_lenght))\n",
    "            if file_lenght==0:\n",
    "                raise ValueError('The csv is empty!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airport is present in the transient state!\n",
      "us_cities is present in the transient state!\n",
      "temperature is present in the transient state!\n",
      "airport is present in the refined state!\n",
      "imigration is present in the refined state!\n",
      "refined/airport_data/airport-codes.csv\n",
      "Csv is fine. Data has 55075 rows\n",
      "refined/imigration_data/sas_data.csv/part-00000-7283a4e0-f32e-4761-a243-0626eeece1b2-c000.csv\n",
      "Csv is fine. Data has 219267 rows\n",
      "refined/imigration_data/sas_data.csv/part-00001-7283a4e0-f32e-4761-a243-0626eeece1b2-c000.csv\n",
      "Csv is fine. Data has 220159 rows\n",
      "refined/imigration_data/sas_data.csv/part-00002-7283a4e0-f32e-4761-a243-0626eeece1b2-c000.csv\n",
      "Csv is fine. Data has 220159 rows\n",
      "refined/imigration_data/sas_data.csv/part-00003-7283a4e0-f32e-4761-a243-0626eeece1b2-c000.csv\n",
      "Csv is fine. Data has 220159 rows\n",
      "refined/imigration_data/sas_data.csv/part-00004-7283a4e0-f32e-4761-a243-0626eeece1b2-c000.csv\n",
      "Csv is fine. Data has 220159 rows\n",
      "refined/imigration_data/sas_data.csv/part-00005-7283a4e0-f32e-4761-a243-0626eeece1b2-c000.csv\n",
      "Csv is fine. Data has 220159 rows\n",
      "refined/imigration_data/sas_data.csv/part-00006-7283a4e0-f32e-4761-a243-0626eeece1b2-c000.csv\n",
      "Csv is fine. Data has 220159 rows\n",
      "refined/imigration_data/sas_data.csv/part-00007-7283a4e0-f32e-4761-a243-0626eeece1b2-c000.csv\n",
      "Csv is fine. Data has 220159 rows\n",
      "refined/imigration_data/sas_data.csv/part-00008-7283a4e0-f32e-4761-a243-0626eeece1b2-c000.csv\n",
      "Csv is fine. Data has 220159 rows\n",
      "refined/imigration_data/sas_data.csv/part-00009-7283a4e0-f32e-4761-a243-0626eeece1b2-c000.csv\n",
      "Csv is fine. Data has 220159 rows\n",
      "refined/imigration_data/sas_data.csv/part-00010-7283a4e0-f32e-4761-a243-0626eeece1b2-c000.csv\n",
      "Csv is fine. Data has 220159 rows\n",
      "refined/imigration_data/sas_data.csv/part-00011-7283a4e0-f32e-4761-a243-0626eeece1b2-c000.csv\n",
      "Csv is fine. Data has 220159 rows\n",
      "refined/imigration_data/sas_data.csv/part-00012-7283a4e0-f32e-4761-a243-0626eeece1b2-c000.csv\n",
      "Csv is fine. Data has 206895 rows\n",
      "transient/airport_data/airport-codes.csv\n",
      "Csv is fine. Data has 55075 rows\n",
      "transient/imigration_data/sas_data.csv/part-00000-a25f511f-1d97-4b5f-8cc0-822f98e665e4-c000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2907: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Csv is fine. Data has 219267 rows\n",
      "transient/imigration_data/sas_data.csv/part-00001-a25f511f-1d97-4b5f-8cc0-822f98e665e4-c000.csv\n",
      "Csv is fine. Data has 220159 rows\n",
      "transient/imigration_data/sas_data.csv/part-00002-a25f511f-1d97-4b5f-8cc0-822f98e665e4-c000.csv\n",
      "Csv is fine. Data has 220159 rows\n",
      "transient/imigration_data/sas_data.csv/part-00003-a25f511f-1d97-4b5f-8cc0-822f98e665e4-c000.csv\n",
      "Csv is fine. Data has 220159 rows\n",
      "transient/imigration_data/sas_data.csv/part-00004-a25f511f-1d97-4b5f-8cc0-822f98e665e4-c000.csv\n",
      "Csv is fine. Data has 220159 rows\n",
      "transient/imigration_data/sas_data.csv/part-00005-a25f511f-1d97-4b5f-8cc0-822f98e665e4-c000.csv\n",
      "Csv is fine. Data has 220159 rows\n",
      "transient/imigration_data/sas_data.csv/part-00006-a25f511f-1d97-4b5f-8cc0-822f98e665e4-c000.csv\n",
      "Csv is fine. Data has 220159 rows\n",
      "transient/imigration_data/sas_data.csv/part-00007-a25f511f-1d97-4b5f-8cc0-822f98e665e4-c000.csv\n",
      "Csv is fine. Data has 220159 rows\n",
      "transient/imigration_data/sas_data.csv/part-00008-a25f511f-1d97-4b5f-8cc0-822f98e665e4-c000.csv\n",
      "Csv is fine. Data has 220159 rows\n",
      "transient/imigration_data/sas_data.csv/part-00009-a25f511f-1d97-4b5f-8cc0-822f98e665e4-c000.csv\n",
      "Csv is fine. Data has 220159 rows\n",
      "transient/imigration_data/sas_data.csv/part-00010-a25f511f-1d97-4b5f-8cc0-822f98e665e4-c000.csv\n",
      "Csv is fine. Data has 220159 rows\n",
      "transient/imigration_data/sas_data.csv/part-00011-a25f511f-1d97-4b5f-8cc0-822f98e665e4-c000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2907: DtypeWarning: Columns (18,23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Csv is fine. Data has 220159 rows\n",
      "transient/imigration_data/sas_data.csv/part-00012-a25f511f-1d97-4b5f-8cc0-822f98e665e4-c000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2907: DtypeWarning: Columns (15,18,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Csv is fine. Data has 206895 rows\n",
      "transient/temperature_data/world_temperature_time_series.csv\n",
      "Csv is fine. Data has 8599212 rows\n",
      "transient/us_cities_data/us_cities_demographics.csv\n",
      "Csv is fine. Data has 2891 rows\n"
     ]
    }
   ],
   "source": [
    "check_files('transient')\n",
    "check_files('refined')\n",
    "check_files_lenght()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Imigration Data\n",
    "\n",
    "1. i94addr - string - the state that receives the immigrant\n",
    "2. biryear - date - birth year of the immigrant\n",
    "3. i94port - string - Aiport of arrival\n",
    "4. arrdate - date - Date of arrival\n",
    "5. gender - date - Gender of the immigrant\n",
    "6. airline - imigrant's airline code \n",
    "\n",
    "##### Temperature Data\n",
    "\n",
    "\n",
    "1. dt - string - Date of temperature\n",
    "2. AverageTemperature - numeric - Average temperature\n",
    "3. AverageTemperatureUncertainty - Uncertainty of temperature\n",
    "4. City - string - The city that has the temperature's mesuare\n",
    "5. Country - string -  The country that has the temperature's mesuare\n",
    "6. Latitude - string - The latitude that has the temperature's mesuare\n",
    "7. Longitude - string - The longitude that has the temperature's mesuare\n",
    "\n",
    "##### US Cities Data\n",
    "\n",
    "1. City - string - USA's city\n",
    "2. State - string - USA's state\n",
    "3. Median Age - numeric - cities's population's median age\n",
    "4. Male Population - numeric - number of male population\n",
    "5. Female Population - numeric - number of female population\n",
    "6. Total Population - int  - number of total population\n",
    "7. Number of Veterans - numeric - number of veterans in that city\n",
    "8. Foreign-born - numeric - number of foreign born in that city\n",
    "9. Average Household Size - numeric - \n",
    "10. State Code - string - the code of the state of the city\n",
    "11. Race - string - the race that is majoritary\n",
    "12. Count - int - count\n",
    "\n",
    "#### Airport Data\n",
    "1. ident - string - Identify the airport\n",
    "2. type - string - Type of the airport (small, medium, large)\n",
    "3. name - string - Name of the Airport\n",
    "4. elevation_ft - numeric - The elevation fit metric\n",
    "5. continent - string - Continent of the airport\n",
    "6. iso_country - string - Country code where the airport is located\n",
    "7. iso_region - string - State code where the airport is located\n",
    "8. municipality - string - City where the airport is located\n",
    "9. gps_code - string - Gps code where the airport is located\n",
    "10. iata_code - string -  Airport code, also known as an IATA location identifier, IATA station code, or simply a location identifier\n",
    "11. local_code - string - Local code where the airport is located\n",
    "12. coordinates - string - Coordinates where the airport is located (latitude, longitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Clearly state the rationale for the choice of tools and technologies for the project:\n",
    "The choice of technology of the datalake was made by some reasons:\n",
    "1. Store cost and storage architeture-  S3 is known by it's cheap cost of storing data. \n",
    "2. Scalability - S3 is also know for it's scalability. According to AWS  \"Amazon S3 provides an optimal foundation for a data lake because of its virtually unlimited scalability. You can seamlessly and nondisruptively increase storage from gigabytes to petabytes of content, paying only for what you use\". \n",
    "3. Standardized APIs - We can easily use python and pyspark framework through the Standardized APIs  to store and retrieve data. \n",
    "4. Dealing with Big Data - As data grows, an ec2 will be necessary. With S3 you can easily integrate both solutions.\n",
    "\n",
    "#### Propose how often the data should be updated and why:\n",
    "Data should be updated monthly. This because, since we are dealing with time series data, we need a substantial period to understand and build a statistical model for the climate change and imigration, since the effect of human action in the case of the climate change data, or economical and political effects in the imigration moviment need's time.\n",
    "\n",
    "###  Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people.\n",
    " \n",
    "#### The data was increased by 100x.\n",
    "In this case, we probably would need an ec2 instance and also use the pyspark framework to deal with all datasets, since the volume would excessed a local machine hability to deal with de data. With s3 we would only need to increse storage.\n",
    "\n",
    "#### The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "S3 has a conector for Powerbi, possibiliting a direct conection every day. The ETL should run on a daily basis, and once this happens, we only need to set the gateway to update de data source everyday. There is also the possibility to use a python script as a data source, retrieving the data with boto for example.\n",
    "\n",
    "####  The database needed to be accessed by 100+ people.\n",
    "Since all the data is public, we could set the S3 bucket also as public. These way, there is no need to create 100 of IAM users."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
